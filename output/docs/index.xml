<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Parquet – Documentation</title><link>/docs/</link><description>Recent content in Documentation on Parquet</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Compression</title><link>/docs/file-format/data-pages/compression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/data-pages/compression/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Parquet allows the data block inside dictionary pages and data pages to
be compressed for better space efficiency. The Parquet format supports
several compression covering different areas in the compression ratio /
processing cost spectrum.&lt;/p>
&lt;p>The detailed specifications of compression codecs are maintained externally
by their respective authors or maintainers, which we reference hereafter.&lt;/p>
&lt;p>For all compression codecs except the deprecated &lt;code>LZ4&lt;/code> codec, the raw data
of a (data or dictionary) page is fed &lt;em>as-is&lt;/em> to the underlying compression
library, without any additional framing or padding. The information required
for precise allocation of compressed and decompressed buffers is written
in the &lt;code>PageHeader&lt;/code> struct.&lt;/p>
&lt;h2 id="codecs">Codecs&lt;/h2>
&lt;h3 id="uncompressed">UNCOMPRESSED&lt;/h3>
&lt;p>No-op codec. Data is left uncompressed.&lt;/p>
&lt;h3 id="snappy">SNAPPY&lt;/h3>
&lt;p>A codec based on the
&lt;a href="https://github.com/google/snappy/blob/master/format_description.txt">Snappy compression format&lt;/a>.
If any ambiguity arises when implementing this format, the implementation
provided by Google Snappy &lt;a href="https://github.com/google/snappy/">library&lt;/a>
is authoritative.&lt;/p>
&lt;h3 id="gzip">GZIP&lt;/h3>
&lt;p>A codec based on the GZIP format (not the closely-related &amp;ldquo;zlib&amp;rdquo; or &amp;ldquo;deflate&amp;rdquo;
formats) defined by &lt;a href="https://tools.ietf.org/html/rfc1952">RFC 1952&lt;/a>.
If any ambiguity arises when implementing this format, the implementation
provided by the &lt;a href="https://zlib.net/">zlib compression library&lt;/a> is authoritative.&lt;/p>
&lt;p>Readers should support reading pages containing multiple GZIP members, however,
as this has historically not been supported by all implementations, it is recommended
that writers refrain from creating such pages by default for better interoperability.&lt;/p>
&lt;h3 id="lzo">LZO&lt;/h3>
&lt;p>A codec based on or interoperable with the
&lt;a href="http://www.oberhumer.com/opensource/lzo/">LZO compression library&lt;/a>.&lt;/p>
&lt;h3 id="brotli">BROTLI&lt;/h3>
&lt;p>A codec based on the Brotli format defined by
&lt;a href="https://tools.ietf.org/html/rfc7932">RFC 7932&lt;/a>.
If any ambiguity arises when implementing this format, the implementation
provided by the &lt;a href="https://github.com/google/brotli">Brotli compression library&lt;/a>
is authoritative.&lt;/p>
&lt;h3 id="lz4">LZ4&lt;/h3>
&lt;p>A &lt;strong>deprecated&lt;/strong> codec loosely based on the LZ4 compression algorithm,
but with an additional undocumented framing scheme. The framing is part
of the original Hadoop compression library and was historically copied
first in parquet-mr, then emulated with mixed results by parquet-cpp.&lt;/p>
&lt;p>It is strongly suggested that implementors of Parquet writers deprecate
this compression codec in their user-facing APIs, and advise users to
switch to the newer, interoperable &lt;code>LZ4_RAW&lt;/code> codec.&lt;/p>
&lt;h3 id="zstd">ZSTD&lt;/h3>
&lt;p>A codec based on the Zstandard format defined by
&lt;a href="https://tools.ietf.org/html/rfc8478">RFC 8478&lt;/a>. If any ambiguity arises
when implementing this format, the implementation provided by the
&lt;a href="https://facebook.github.io/zstd/">ZStandard compression library&lt;/a>
is authoritative.&lt;/p>
&lt;h3 id="lz4_raw">LZ4_RAW&lt;/h3>
&lt;p>A codec based on the &lt;a href="https://github.com/lz4/lz4/blob/dev/doc/lz4_Block_format.md">LZ4 block format&lt;/a>.
If any ambiguity arises when implementing this format, the implementation
provided by the &lt;a href="http://www.lz4.org/">LZ4 compression library&lt;/a> is authoritative.&lt;/p></description></item><item><title>Docs: Encodings</title><link>/docs/file-format/data-pages/encodings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/data-pages/encodings/</guid><description>
&lt;p>&lt;a name="PLAIN">&lt;/a>&lt;/p>
&lt;h3 id="plain-plain--0">Plain: (PLAIN = 0)&lt;/h3>
&lt;p>Supported Types: all&lt;/p>
&lt;p>This is the plain encoding that must be supported for types. It is
intended to be the simplest encoding. Values are encoded back to back.&lt;/p>
&lt;p>The plain encoding is used whenever a more efficient encoding can not be used. It
stores the data in the following format:&lt;/p>
&lt;ul>
&lt;li>BOOLEAN: &lt;a href="/docs/file-format/data-pages/encodings/#BITPACKED">Bit Packed&lt;/a>, LSB first&lt;/li>
&lt;li>INT32: 4 bytes little endian&lt;/li>
&lt;li>INT64: 8 bytes little endian&lt;/li>
&lt;li>INT96: 12 bytes little endian (deprecated)&lt;/li>
&lt;li>FLOAT: 4 bytes IEEE little endian&lt;/li>
&lt;li>DOUBLE: 8 bytes IEEE little endian&lt;/li>
&lt;li>BYTE_ARRAY: length in 4 bytes little endian followed by the bytes contained in the array&lt;/li>
&lt;li>FIXED_LEN_BYTE_ARRAY: the bytes contained in the array&lt;/li>
&lt;/ul>
&lt;p>For native types, this outputs the data as little endian. Floating
point types are encoded in IEEE.&lt;/p>
&lt;p>For the byte array type, it encodes the length as a 4 byte little
endian, followed by the bytes.&lt;/p>
&lt;h3 id="dictionary-encoding-plain_dictionary--2-and-rle_dictionary--8">Dictionary Encoding (PLAIN_DICTIONARY = 2 and RLE_DICTIONARY = 8)&lt;/h3>
&lt;p>The dictionary encoding builds a dictionary of values encountered in a given column. The
dictionary will be stored in a dictionary page per column chunk. The values are stored as integers
using the &lt;a href="/docs/file-format/data-pages/encodings/#RLE">RLE/Bit-Packing Hybrid&lt;/a> encoding. If the dictionary grows too big, whether in size
or number of distinct values, the encoding will fall back to the plain encoding. The dictionary page is
written first, before the data pages of the column chunk.&lt;/p>
&lt;p>Dictionary page format: the entries in the dictionary using the &lt;a href="/docs/file-format/data-pages/encodings/#PLAIN">plain&lt;/a> encoding.&lt;/p>
&lt;p>Data page format: the bit width used to encode the entry ids stored as 1 byte (max bit width = 32),
followed by the values encoded using RLE/Bit packed described above (with the given bit width).&lt;/p>
&lt;p>Using the PLAIN_DICTIONARY enum value is deprecated in the Parquet 2.0 specification. Prefer using RLE_DICTIONARY
in a data page and PLAIN in a dictionary page for Parquet 2.0+ files.&lt;/p>
&lt;p>&lt;a name="RLE">&lt;/a>&lt;/p>
&lt;h3 id="run-length-encoding--bit-packing-hybrid-rle--3">Run Length Encoding / Bit-Packing Hybrid (RLE = 3)&lt;/h3>
&lt;p>This encoding uses a combination of bit-packing and run length encoding to more efficiently store repeated values.&lt;/p>
&lt;p>The grammar for this encoding looks like this, given a fixed bit-width known in advance:&lt;/p>
&lt;pre tabindex="0">&lt;code>rle-bit-packed-hybrid: &amp;lt;length&amp;gt; &amp;lt;encoded-data&amp;gt;
// length is not always prepended, please check the table below for more detail
length := length of the &amp;lt;encoded-data&amp;gt; in bytes stored as 4 bytes little endian (unsigned int32)
encoded-data := &amp;lt;run&amp;gt;*
run := &amp;lt;bit-packed-run&amp;gt; | &amp;lt;rle-run&amp;gt;
bit-packed-run := &amp;lt;bit-packed-header&amp;gt; &amp;lt;bit-packed-values&amp;gt;
bit-packed-header := varint-encode(&amp;lt;bit-pack-scaled-run-len&amp;gt; &amp;lt;&amp;lt; 1 | 1)
// we always bit-pack a multiple of 8 values at a time, so we only store the number of values / 8
bit-pack-scaled-run-len := (bit-packed-run-len) / 8
bit-packed-run-len := *see 3 below*
bit-packed-values := *see 1 below*
rle-run := &amp;lt;rle-header&amp;gt; &amp;lt;repeated-value&amp;gt;
rle-header := varint-encode( (rle-run-len) &amp;lt;&amp;lt; 1)
rle-run-len := *see 3 below*
repeated-value := value that is repeated, using a fixed-width of round-up-to-next-byte(bit-width)
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>
&lt;p>The bit-packing here is done in a different order than the one in the &lt;a href="/docs/file-format/data-pages/encodings/#BITPACKED">deprecated bit-packing&lt;/a> encoding.
The values are packed from the least significant bit of each byte to the most significant bit,
though the order of the bits in each value remains in the usual order of most significant to least
significant. For example, to pack the same values as the example in the deprecated encoding above:&lt;/p>
&lt;p>The numbers 1 through 7 using bit width 3:&lt;/p>
&lt;pre tabindex="0">&lt;code>dec value: 0 1 2 3 4 5 6 7
bit value: 000 001 010 011 100 101 110 111
bit label: ABC DEF GHI JKL MNO PQR STU VWX
&lt;/code>&lt;/pre>&lt;p>would be encoded like this where spaces mark byte boundaries (3 bytes):&lt;/p>
&lt;pre tabindex="0">&lt;code>bit value: 10001000 11000110 11111010
bit label: HIDEFABC RMNOJKLG VWXSTUPQ
&lt;/code>&lt;/pre>&lt;p>The reason for this packing order is to have fewer word-boundaries on little-endian hardware
when deserializing more than one byte at at time. This is because 4 bytes can be read into a
32 bit register (or 8 bytes into a 64 bit register) and values can be unpacked just by
shifting and ORing with a mask. (to make this optimization work on a big-endian machine,
you would have to use the ordering used in the &lt;a href="/docs/file-format/data-pages/encodings/#BITPACKED">deprecated bit-packing&lt;/a> encoding)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>varint-encode() is ULEB-128 encoding, see &lt;a href="https://en.wikipedia.org/wiki/LEB128">https://en.wikipedia.org/wiki/LEB128&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bit-packed-run-len and rle-run-len must be in the range [1, 2&lt;sup>31&lt;/sup> - 1].
This means that a Parquet implementation can always store the run length in a signed
32-bit integer. This length restriction was not part of the Parquet 2.5.0 and earlier
specifications, but longer runs were not readable by the most common Parquet
implementations so, in practice, were not safe for Parquet writers to emit.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Note that the RLE encoding method is only supported for the following types of
data:&lt;/p>
&lt;ul>
&lt;li>Repetition and definition levels&lt;/li>
&lt;li>Dictionary indices&lt;/li>
&lt;li>Boolean values in data pages, as an alternative to PLAIN encoding&lt;/li>
&lt;/ul>
&lt;p>Whether prepending the four-byte &lt;code>length&lt;/code> to the &lt;code>encoded-data&lt;/code> is summarized as the table below:&lt;/p>
&lt;pre tabindex="0">&lt;code>+--------------+------------------------+-----------------+
| Page kind | RLE-encoded data kind | Prepend length? |
+--------------+------------------------+-----------------+
| Data page v1 | Definition levels | Y |
| | Repetition levels | Y |
| | Dictionary indices | N |
| | Boolean values | Y |
+--------------+------------------------+-----------------+
| Data page v2 | Definition levels | N |
| | Repetition levels | N |
| | Dictionary indices | N |
| | Boolean values | Y |
+--------------+------------------------+-----------------+
&lt;/code>&lt;/pre>&lt;p>&lt;a name="BITPACKED">&lt;/a>&lt;/p>
&lt;h3 id="bit-packed-deprecated-bit_packed--4">Bit-packed (Deprecated) (BIT_PACKED = 4)&lt;/h3>
&lt;p>This is a bit-packed only encoding, which is deprecated and will be replaced by the &lt;a href="/docs/file-format/data-pages/encodings/#RLE">RLE/bit-packing&lt;/a> hybrid encoding.
Each value is encoded back to back using a fixed width.
There is no padding between values (except for the last byte, which is padded with 0s).
For example, if the max repetition level was 3 (2 bits) and the max definition level as 3
(2 bits), to encode 30 values, we would have 30 * 2 = 60 bits = 8 bytes.&lt;/p>
&lt;p>This implementation is deprecated because the &lt;a href="/docs/file-format/data-pages/encodings/#RLE">RLE/bit-packing&lt;/a> hybrid is a superset of this implementation.
For compatibility reasons, this implementation packs values from the most significant bit to the least significant bit,
which is not the same as the &lt;a href="/docs/file-format/data-pages/encodings/#RLE">RLE/bit-packing&lt;/a> hybrid.&lt;/p>
&lt;p>For example, the numbers 1 through 7 using bit width 3:&lt;/p>
&lt;pre tabindex="0">&lt;code>dec value: 0 1 2 3 4 5 6 7
bit value: 000 001 010 011 100 101 110 111
bit label: ABC DEF GHI JKL MNO PQR STU VWX
&lt;/code>&lt;/pre>&lt;p>would be encoded like this where spaces mark byte boundaries (3 bytes):&lt;/p>
&lt;pre tabindex="0">&lt;code>bit value: 00000101 00111001 01110111
bit label: ABCDEFGH IJKLMNOP QRSTUVWX
&lt;/code>&lt;/pre>&lt;p>Note that the BIT_PACKED encoding method is only supported for encoding
repetition and definition levels.&lt;/p>
&lt;p>&lt;a name="DELTAENC">&lt;/a>&lt;/p>
&lt;h3 id="delta-encoding-delta_binary_packed--5">Delta Encoding (DELTA_BINARY_PACKED = 5)&lt;/h3>
&lt;p>Supported Types: INT32, INT64&lt;/p>
&lt;p>This encoding is adapted from the Binary packing described in
&lt;a href="http://arxiv.org/pdf/1209.2137v5.pdf">&amp;ldquo;Decoding billions of integers per second through vectorization&amp;rdquo;&lt;/a>
by D. Lemire and L. Boytsov.&lt;/p>
&lt;p>In delta encoding we make use of variable length integers for storing various
numbers (not the deltas themselves). For unsigned values, we use ULEB128,
which is the unsigned version of LEB128 (&lt;a href="https://en.wikipedia.org/wiki/LEB128#Unsigned_LEB128)">https://en.wikipedia.org/wiki/LEB128#Unsigned_LEB128)&lt;/a>.
For signed values, we use zigzag encoding (&lt;a href="https://developers.google.com/protocol-buffers/docs/encoding#signed-integers">https://developers.google.com/protocol-buffers/docs/encoding#signed-integers&lt;/a>)
to map negative values to positive ones and apply ULEB128 on the result.&lt;/p>
&lt;p>Delta encoding consists of a header followed by blocks of delta encoded values
binary packed. Each block is made of miniblocks, each of them binary packed with its own bit width.&lt;/p>
&lt;p>The header is defined as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;block size in values&amp;gt; &amp;lt;number of miniblocks in a block&amp;gt; &amp;lt;total value count&amp;gt; &amp;lt;first value&amp;gt;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>the block size is a multiple of 128; it is stored as a ULEB128 int&lt;/li>
&lt;li>the miniblock count per block is a divisor of the block size such that their
quotient, the number of values in a miniblock, is a multiple of 32; it is
stored as a ULEB128 int&lt;/li>
&lt;li>the total value count is stored as a ULEB128 int&lt;/li>
&lt;li>the first value is stored as a zigzag ULEB128 int&lt;/li>
&lt;/ul>
&lt;p>Each block contains&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;min delta&amp;gt; &amp;lt;list of bitwidths of miniblocks&amp;gt; &amp;lt;miniblocks&amp;gt;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>the min delta is a zigzag ULEB128 int (we compute a minimum as we need
positive integers for bit packing)&lt;/li>
&lt;li>the bitwidth of each block is stored as a byte&lt;/li>
&lt;li>each miniblock is a list of bit packed ints according to the bit width
stored at the begining of the block&lt;/li>
&lt;/ul>
&lt;p>To encode a block, we will:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Compute the differences between consecutive elements. For the first
element in the block, use the last element in the previous block or, in
the case of the first block, use the first value of the whole sequence,
stored in the header.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Compute the frame of reference (the minimum of the deltas in the block).
Subtract this min delta from all deltas in the block. This guarantees that
all values are non-negative.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Encode the frame of reference (min delta) as a zigzag ULEB128 int followed
by the bit widths of the miniblocks and the delta values (minus the min
delta) bit-packed per miniblock.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Having multiple blocks allows us to adapt to changes in the data by changing
the frame of reference (the min delta) which can result in smaller values
after the subtraction which, again, means we can store them with a lower bit width.&lt;/p>
&lt;p>If there are not enough values to fill the last miniblock, we pad the miniblock
so that its length is always the number of values in a full miniblock multiplied
by the bit width. The values of the padding bits should be zero, but readers
must accept paddings consisting of arbitrary bits as well.&lt;/p>
&lt;p>If, in the last block, less than &lt;code>&amp;lt;number of miniblocks in a block&amp;gt;&lt;/code>
miniblocks are needed to store the values, the bytes storing the bit widths
of the unneeded miniblocks are still present, their value should be zero,
but readers must accept arbitrary values as well. There are no additional
padding bytes for the miniblock bodies though, as if their bit widths were 0
(regardless of the actual byte values). The reader knows when to stop reading
by keeping track of the number of values read.&lt;/p>
&lt;p>Subtractions in steps 1) and 2) may incur signed arithmetic overflow, and so
will the corresponding additions when decoding. Overflow should be allowed
and handled as wrapping around in 2&amp;rsquo;s complement notation so that the original
values are correctly restituted. This may require explicit care in some programming
languages (for example by doing all arithmetic in the unsigned domain).&lt;/p>
&lt;p>The following examples use 8 as the block size to keep the examples short,
but in real cases it would be invalid.&lt;/p>
&lt;h4 id="example-1">Example 1&lt;/h4>
&lt;p>1, 2, 3, 4, 5&lt;/p>
&lt;p>After step 1), we compute the deltas as:&lt;/p>
&lt;p>1, 1, 1, 1&lt;/p>
&lt;p>The minimum delta is 1 and after step 2, the relative deltas become:&lt;/p>
&lt;p>0, 0, 0, 0&lt;/p>
&lt;p>The final encoded data is:&lt;/p>
&lt;p>header:
8 (block size), 1 (miniblock count), 5 (value count), 1 (first value)&lt;/p>
&lt;p>block:
1 (minimum delta), 0 (bitwidth), (no data needed for bitwidth 0)&lt;/p>
&lt;h4 id="example-2">Example 2&lt;/h4>
&lt;p>7, 5, 3, 1, 2, 3, 4, 5, the deltas would be&lt;/p>
&lt;p>-2, -2, -2, 1, 1, 1, 1&lt;/p>
&lt;p>The minimum is -2, so the relative deltas are:&lt;/p>
&lt;p>0, 0, 0, 3, 3, 3, 3&lt;/p>
&lt;p>The encoded data is&lt;/p>
&lt;p>header:
8 (block size), 1 (miniblock count), 8 (value count), 7 (first value)&lt;/p>
&lt;p>block:
-2 (minimum delta), 2 (bitwidth), 00000011111111b (0,0,0,3,3,3,3 packed on 2 bits)&lt;/p>
&lt;h4 id="characteristics">Characteristics&lt;/h4>
&lt;p>This encoding is similar to the &lt;a href="/docs/file-format/data-pages/encodings/#RLE">RLE/bit-packing&lt;/a> encoding. However the &lt;a href="/docs/file-format/data-pages/encodings/#RLE">RLE/bit-packing&lt;/a> encoding is specifically used when the range of ints is small over the entire page, as is true of repetition and definition levels. It uses a single bit width for the whole page.
The delta encoding algorithm described above stores a bit width per miniblock and is less sensitive to variations in the size of encoded integers. It is also somewhat doing RLE encoding as a block containing all the same values will be bit packed to a zero bit width thus being only a header.&lt;/p>
&lt;h3 id="delta-length-byte-array-delta_length_byte_array--6">Delta-length byte array: (DELTA_LENGTH_BYTE_ARRAY = 6)&lt;/h3>
&lt;p>Supported Types: BYTE_ARRAY&lt;/p>
&lt;p>This encoding is always preferred over PLAIN for byte array columns.&lt;/p>
&lt;p>For this encoding, we will take all the byte array lengths and encode them using delta
encoding (DELTA_BINARY_PACKED). The byte array data follows all of the length data just
concatenated back to back. The expected savings is from the cost of encoding the lengths
and possibly better compression in the data (it is no longer interleaved with the lengths).&lt;/p>
&lt;p>The data stream looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;Delta Encoded Lengths&amp;gt; &amp;lt;Byte Array Data&amp;gt;
&lt;/code>&lt;/pre>&lt;p>For example, if the data was &amp;ldquo;Hello&amp;rdquo;, &amp;ldquo;World&amp;rdquo;, &amp;ldquo;Foobar&amp;rdquo;, &amp;ldquo;ABCDEF&amp;rdquo;&lt;/p>
&lt;p>then the encoded data would be comprised of the following segments:&lt;/p>
&lt;ul>
&lt;li>DeltaEncoding(5, 5, 6, 6) (the string lengths)&lt;/li>
&lt;li>&amp;ldquo;HelloWorldFoobarABCDEF&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h3 id="delta-strings-delta_byte_array--7">Delta Strings: (DELTA_BYTE_ARRAY = 7)&lt;/h3>
&lt;p>Supported Types: BYTE_ARRAY, FIXED_LEN_BYTE_ARRAY&lt;/p>
&lt;p>This is also known as incremental encoding or front compression: for each element in a
sequence of strings, store the prefix length of the previous entry plus the suffix.&lt;/p>
&lt;p>For a longer description, see &lt;a href="https://en.wikipedia.org/wiki/Incremental_encoding">https://en.wikipedia.org/wiki/Incremental_encoding&lt;/a>.&lt;/p>
&lt;p>This is stored as a sequence of delta-encoded prefix lengths (DELTA_BINARY_PACKED), followed by
the suffixes encoded as delta length byte arrays (DELTA_LENGTH_BYTE_ARRAY).&lt;/p>
&lt;p>For example, if the data was &amp;ldquo;axis&amp;rdquo;, &amp;ldquo;axle&amp;rdquo;, &amp;ldquo;babble&amp;rdquo;, &amp;ldquo;babyhood&amp;rdquo;&lt;/p>
&lt;p>then the encoded data would be comprised of the following segments:&lt;/p>
&lt;ul>
&lt;li>DeltaEncoding(0, 2, 0, 3) (the prefix lengths)&lt;/li>
&lt;li>DeltaEncoding(4, 2, 6, 5) (the suffix lengths)&lt;/li>
&lt;li>&amp;ldquo;axislebabbleyhood&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>Note that, even for FIXED_LEN_BYTE_ARRAY, all lengths are encoded despite the redundancy.&lt;/p>
&lt;h3 id="byte-stream-split-byte_stream_split--9">Byte Stream Split: (BYTE_STREAM_SPLIT = 9)&lt;/h3>
&lt;p>Supported Types: FLOAT, DOUBLE&lt;/p>
&lt;p>This encoding does not reduce the size of the data but can lead to a significantly better
compression ratio and speed when a compression algorithm is used afterwards.&lt;/p>
&lt;p>This encoding creates K byte-streams of length N where K is the size in bytes of the data
type and N is the number of elements in the data sequence. Specifically, K is 4 for FLOAT
type and 8 for DOUBLE type.
The bytes of each value are scattered to the corresponding streams. The 0-th byte goes to the
0-th stream, the 1-st byte goes to the 1-st stream and so on.
The streams are concatenated in the following order: 0-th stream, 1-st stream, etc.
The total length of encoded streams is K * N bytes. Because it does not have any metadata
to indicate the total length, the end of the streams is also the end of data page. No padding
is allowed inside the data page.&lt;/p>
&lt;p>Example:
Original data is three 32-bit floats and for simplicity we look at their raw representation.&lt;/p>
&lt;pre tabindex="0">&lt;code> Element 0 Element 1 Element 2
Bytes AA BB CC DD 00 11 22 33 A3 B4 C5 D6
&lt;/code>&lt;/pre>&lt;p>After applying the transformation, the data has the following representation:&lt;/p>
&lt;pre tabindex="0">&lt;code>Bytes AA 00 A3 BB 11 B4 CC 22 C5 DD 33 D6
&lt;/code>&lt;/pre></description></item><item><title>Docs: Parquet Modular Encryption</title><link>/docs/file-format/data-pages/encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/data-pages/encryption/</guid><description>
&lt;p>Parquet files containing sensitive information can be protected by the modular encryption
mechanism that encrypts and authenticates the file data and metadata - while allowing
for a regular Parquet functionality (columnar projection, predicate pushdown, encoding
and compression).&lt;/p>
&lt;h2 id="1-problem-statement">1 Problem Statement&lt;/h2>
&lt;p>Existing data protection solutions (such as flat encryption of files, in-storage encryption,
or use of an encrypting storage client) can be applied to Parquet files, but have various
security or performance issues. An encryption mechanism, integrated in the Parquet format,
allows for an optimal combination of data security, processing speed and encryption granularity.&lt;/p>
&lt;h2 id="2-goals">2 Goals&lt;/h2>
&lt;ol>
&lt;li>Protect Parquet data and metadata by encryption, while enabling selective reads
(columnar projection, predicate push-down).&lt;/li>
&lt;li>Implement &amp;ldquo;client-side&amp;rdquo; encryption/decryption (storage client). The storage server
must not see plaintext data, metadata or encryption keys.&lt;/li>
&lt;li>Leverage authenticated encryption that allows clients to check integrity of the retrieved
data - making sure the file (or file parts) have not been replaced with a wrong version, or
tampered with otherwise.&lt;/li>
&lt;li>Enable different encryption keys for different columns and for the footer.&lt;/li>
&lt;li>Allow for partial encryption - encrypt only column(s) with sensitive data.&lt;/li>
&lt;li>Work with all compression and encoding mechanisms supported in Parquet.&lt;/li>
&lt;li>Support multiple encryption algorithms, to account for different security and performance
requirements.&lt;/li>
&lt;li>Enable two modes for metadata protection -
&lt;ul>
&lt;li>full protection of file metadata&lt;/li>
&lt;li>partial protection of file metadata that allows legacy readers to access unencrypted
columns in an encrypted file.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Minimize overhead of encryption - in terms of size of encrypted files, and throughput
of write/read operations.&lt;/li>
&lt;/ol>
&lt;h2 id="3-technical-approach">3 Technical Approach&lt;/h2>
&lt;p>Parquet files are comprised of separately serialized components: pages, page headers, column
indexes, offset indexes, bloom filter headers and bitsets, the footer. Parquet encryption
mechanism denotes them as “modules”
and encrypts each module separately – making it possible to fetch and decrypt the footer,
find the offset of required pages, fetch the pages and decrypt the data. In this document,
the term “footer” always refers to the regular Parquet footer - the &lt;code>FileMetaData&lt;/code> structure,
and its nested fields (row groups / column chunks).&lt;/p>
&lt;p>File encryption is flexible - each column and the footer can be encrypted with the same key,
with a different key, or not encrypted at all.&lt;/p>
&lt;p>The results of compression of column pages are encrypted before being written to the output
stream. A new Thrift structure, with column crypto metadata, is added to column chunks of
the encrypted columns. This metadata provides information about the column encryption keys.&lt;/p>
&lt;p>The results of serialization of Thrift structures are encrypted, before being written
to the output stream.&lt;/p>
&lt;p>The file footer can be either encrypted or left as a plaintext. In an encrypted footer mode,
a new Thrift structure with file crypto metadata is added to the file. This metadata provides
information about the file encryption algorithm and the footer encryption key.&lt;/p>
&lt;p>In a plaintext footer mode, the contents of the footer structure is visible and signed
in order to verify its integrity. New footer fields keep an
information about the file encryption algorithm and the footer signing key.&lt;/p>
&lt;p>For encrypted columns, the following modules are always encrypted, with the same column key:
pages and page headers (both dictionary and data), column indexes, offset indexes, bloom filter
headers and bitsets. If the
column key is different from the footer encryption key, the column metadata is serialized
separately and encrypted with the column key. In this case, the column metadata is also
considered to be a module.&lt;/p>
&lt;h2 id="4-encryption-algorithms-and-keys">4 Encryption Algorithms and Keys&lt;/h2>
&lt;p>Parquet encryption algorithms are based on the standard AES ciphers for symmetric encryption.
AES is supported in Intel and other CPUs with hardware acceleration of crypto operations
(“AES-NI”) - that can be leveraged, for example, by Java programs (automatically via HotSpot),
or C++ programs (via EVP-* functions in OpenSSL). Parquet supports all standard AES key sizes:
128, 192 and 256 bits.&lt;/p>
&lt;p>Initially, two algorithms have been implemented, one based on a GCM mode of AES, and the
other on a combination of GCM and CTR modes.&lt;/p>
&lt;h3 id="41-aes-modes-used-in-parquet">4.1 AES modes used in Parquet&lt;/h3>
&lt;h4 id="411-aes-gcm">4.1.1 AES GCM&lt;/h4>
&lt;p>AES GCM is an authenticated encryption. Besides the data confidentiality (encryption), it
supports two levels of integrity verification (authentication): of the data (default),
and of the data combined with an optional AAD (“additional authenticated data”). The
authentication allows to make sure the data has not been tampered with. An AAD
is a free text to be authenticated, together with the data. The user can, for example, pass the
file name with its version (or creation timestamp) as an AAD input, to verify that the
file has not been replaced with an older version. The details on how Parquet creates
and uses AADs are provided in the section 4.4.&lt;/p>
&lt;h4 id="412-aes-ctr">4.1.2 AES CTR&lt;/h4>
&lt;p>AES CTR is a regular (not authenticated) cipher. It is faster than the GCM cipher, since it
doesn’t perform integrity verification and doesn’t calculate an authentication tag.
Actually, GCM is a combination of the CTR cipher and an
authentication layer called GMAC. For applications running without AES acceleration
(e.g. on Java versions before Java 9) and willing to compromise on content verification,
CTR cipher can provide a boost in encryption/decryption throughput.&lt;/p>
&lt;h4 id="413-nonces-and-ivs">4.1.3 Nonces and IVs&lt;/h4>
&lt;p>GCM and CTR ciphers require a unique vector to be provided for each encrypted stream.
In this document, the unique input to GCM encryption is called nonce (“number used once”).
The unique input to CTR encryption is called IV (&amp;ldquo;initialization vector&amp;rdquo;), and is comprised of two
parts: a nonce and an initial counter field.&lt;/p>
&lt;p>Parquet encryption uses the RBG-based (random bit generator) nonce construction as defined in
the section 8.2.2 of the NIST SP 800-38D document. For each encrypted module, Parquet generates a
unique nonce with a length of 12 bytes (96 bits). Notice: the NIST
specification uses a term “IV” for what is called “nonce” in the Parquet encryption design.&lt;/p>
&lt;h3 id="42-parquet-encryption-algorithms">4.2 Parquet encryption algorithms&lt;/h3>
&lt;h4 id="421-aes_gcm_v1">4.2.1 AES_GCM_V1&lt;/h4>
&lt;p>This Parquet algorithm encrypts all modules by the GCM cipher, without padding. The AES GCM cipher
must be implemented by a cryptographic provider according to the NIST SP 800-38D specification.&lt;/p>
&lt;p>In Parquet, an input to the GCM cipher is an encryption key, a 12-byte nonce, a plaintext and an
AAD. The output is a ciphertext with the length equal to that of plaintext, and a 16-byte authentication
tag used to verify the ciphertext and AAD integrity.&lt;/p>
&lt;h4 id="422-aes_gcm_ctr_v1">4.2.2 AES_GCM_CTR_V1&lt;/h4>
&lt;p>In this Parquet algorithm, all modules except pages are encrypted with the GCM cipher, as described
above. The pages are encrypted by the CTR cipher without padding. This allows to encrypt/decrypt
the bulk of the data faster, while still verifying the metadata integrity and making
sure the file has not been replaced with a wrong version. However, tampering with the
page data might go unnoticed. The AES CTR cipher
must be implemented by a cryptographic provider according to the NIST SP 800-38A specification.&lt;/p>
&lt;p>In Parquet, an input to the CTR cipher is an encryption key, a 16-byte IV and a plaintext. IVs are comprised of
a 12-byte nonce and a 4-byte initial counter field. The first 31 bits of the initial counter field are set
to 0, the last bit is set to 1. The output is a ciphertext with the length equal to that of plaintext.&lt;/p>
&lt;h3 id="43-key-metadata">4.3 Key metadata&lt;/h3>
&lt;p>A wide variety of services and tools for management of encryption keys exist in the
industry today. Public clouds offer different key management services (KMS), and
organizational IT systems either build proprietary key managers in-house or adopt open source
tools for on-premises deployment. Besides the diversity of management tools, there are many
ways to generate and handle the keys themselves (generate Data keys inside KMS – or locally
upon data encryption; use Data keys only, or use Master keys to encrypt the Data keys;
store the encrypted key material inside the data file, or at a separate location; etc). There
is also a large variety of authorization and certification methods, required to control the
access to encryption keys.&lt;/p>
&lt;p>Parquet is not limited to a single KMS, key generation/wrapping method, or authorization service.
Instead, Parquet provides a developer with a simple interface that can be utilized for implementation
of any key management scheme. For each column or footer key, a file writer can generate and pass an
arbitrary &lt;code>key_metadata&lt;/code> byte array that will be stored in the file. This field is made available to
file readers to enable recovery of the key. For example, the key_metadata
can keep a serialized&lt;/p>
&lt;ul>
&lt;li>String ID of a Data key. This enables direct retrieval of the Data key from a KMS.&lt;/li>
&lt;li>Encrypted Data key, and string ID of a Master key. The Data key is generated randomly and
encrypted with a Master key either remotely in a KMS, or locally after retrieving the Master key from a KMS.
Master key rotation requires modification of the data file footer.&lt;/li>
&lt;li>Short ID (counter) of a Data key inside the Parquet data file. The Data key is encrypted with a
Master key using one of the options described above – but the resulting key material is stored
separately, outside the data file, and will be retrieved using the counter and file path.
Master key rotation doesn&amp;rsquo;t require modification of the data file.&lt;/li>
&lt;/ul>
&lt;p>Key metadata can also be empty - in a case the encryption keys are fully managed by the caller
code, and passed explicitly to Parquet readers for the file footer and each encrypted column.&lt;/p>
&lt;h3 id="44-additional-authenticated-data">4.4 Additional Authenticated Data&lt;/h3>
&lt;p>The AES GCM cipher protects against byte replacement inside a ciphertext - but, without an AAD,
it can&amp;rsquo;t prevent replacement of one ciphertext with another (encrypted with the same key).
Parquet modular encryption leverages AADs to protect against swapping ciphertext modules (encrypted
with AES GCM) inside a file or between files. Parquet can also protect against swapping full
files - for example, replacement of a file with an old version, or replacement of one table
partition with another. AADs are built to reflects the identity of a file and of the modules
inside the file.&lt;/p>
&lt;p>Parquet constructs a module AAD from two components: an optional AAD prefix - a string provided
by the user for the file, and an AAD suffix, built internally for each GCM-encrypted module
inside the file. The AAD prefix should reflect the target identity that helps to detect file
swapping (a simple example - table name with a date and partition, e.g. &amp;ldquo;employees_23May2018.part0&amp;rdquo;).
The AAD suffix reflects the internal identity of modules inside the file, which for example
prevents replacement of column pages in row group 0 by pages from the same column in row
group 1. The module AAD is a direct concatenation of the prefix and suffix parts.&lt;/p>
&lt;h4 id="441-aad-prefix">4.4.1 AAD prefix&lt;/h4>
&lt;p>File swapping can be prevented by an AAD prefix string, that uniquely identifies the file and
allows to differentiate it e.g. from older versions of the file or from other partition files in the same
data set (table). This string is optionally passed by a writer upon file creation. If provided,
the AAD prefix is stored in an &lt;code>aad_prefix&lt;/code> field in the file, and is made available to the readers.
This field is not encrypted. If a user is concerned about keeping the file identity inside the file,
the writer code can explicitly request Parquet not to store the AAD prefix. Then the aad_prefix field
will be empty; AAD prefixes must be fully managed by the caller code and supplied explictly to Parquet
readers for each file.&lt;/p>
&lt;p>The protection against swapping full files is optional. It is not enabled by default because
it requires the writers to generate and pass an AAD prefix.&lt;/p>
&lt;p>A reader of a file created with an AAD prefix, should be able to verify the prefix (file identity)
by comparing it with e.g. the target table name, using a convention accepted in the organization.
Readers of data sets, comprised of multiple partition files, can verify data set integrity by
checking the number of files and the AAD prefix of each file. For example, a reader that needs to
process the employee table, a May 23 version, knows (via the convention) that
the AAD prefix must be &amp;ldquo;employees_23May2018.partN&amp;rdquo; in
each corresponding table file. If a file AAD prefix is &amp;ldquo;employees_23May2018.part0&amp;rdquo;, the reader
will know it is fine, but if the prefix is &amp;ldquo;employees_23May2016.part0&amp;rdquo; or &amp;ldquo;contractors_23May2018.part0&amp;rdquo; -
the file is wrong. The reader should also know the number of table partitions and verify availability
of all partition files (prefixes) from 0 to N-1.&lt;/p>
&lt;h4 id="442-aad-suffix">4.4.2 AAD suffix&lt;/h4>
&lt;p>The suffix part of a module AAD protects against module swapping inside a file. It also protects against
module swapping between files - in situations when an encryption key is re-used in multiple files and the
writer has not provided a unique AAD prefix for each file.&lt;/p>
&lt;p>Unlike AAD prefix, a suffix is built internally by Parquet, by direct concatenation of the following parts:&lt;/p>
&lt;ol>
&lt;li>[All modules] internal file identifier - a random byte array generated for each file (implementation-defined length)&lt;/li>
&lt;li>[All modules] module type (1 byte)&lt;/li>
&lt;li>[All modules except footer] row group ordinal (2 byte short, little endian)&lt;/li>
&lt;li>[All modules except footer] column ordinal (2 byte short, little endian)&lt;/li>
&lt;li>[Data page and header only] page ordinal (2 byte short, little endian)&lt;/li>
&lt;/ol>
&lt;p>The following module types are defined:&lt;/p>
&lt;ul>
&lt;li>Footer (0)&lt;/li>
&lt;li>ColumnMetaData (1)&lt;/li>
&lt;li>Data Page (2)&lt;/li>
&lt;li>Dictionary Page (3)&lt;/li>
&lt;li>Data PageHeader (4)&lt;/li>
&lt;li>Dictionary PageHeader (5)&lt;/li>
&lt;li>ColumnIndex (6)&lt;/li>
&lt;li>OffsetIndex (7)&lt;/li>
&lt;li>BloomFilter Header (8)&lt;/li>
&lt;li>BloomFilter Bitset (9)&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Internal File ID&lt;/th>
&lt;th>Module type&lt;/th>
&lt;th>Row group ordinal&lt;/th>
&lt;th>Column ordinal&lt;/th>
&lt;th>Page ordinal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Footer&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (0)&lt;/td>
&lt;td>no&lt;/td>
&lt;td>no&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ColumnMetaData&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (1)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Data Page&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (2)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dictionary Page&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (3)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Data PageHeader&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (4)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dictionary PageHeader&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (5)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ColumnIndex&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (6)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OffsetIndex&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (7)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BloomFilter Header&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (8)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BloomFilter Bitset&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes (9)&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="5-file-format">5 File Format&lt;/h2>
&lt;h3 id="51-encrypted-module-serialization">5.1 Encrypted module serialization&lt;/h3>
&lt;p>All modules, except column pages, are encrypted with the GCM cipher. In the AES_GCM_V1 algorithm,
the column pages are also encrypted with AES GCM. For each module, the GCM encryption
buffer is comprised of a nonce, ciphertext and tag, described in the Algorithms section. The length of
the encryption buffer (a 4-byte little endian) is written to the output stream, followed by the buffer itself.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>length (4 bytes)&lt;/th>
&lt;th>nonce (12 bytes)&lt;/th>
&lt;th>ciphertext (length-28 bytes)&lt;/th>
&lt;th>tag (16 bytes)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>In the AES_GCM_CTR_V1 algorithm, the column pages are encrypted with AES CTR.
For each page, the CTR encryption buffer is comprised of a nonce and ciphertext,
described in the Algorithms section. The length of the encryption buffer
(a 4-byte little endian) is written to the output stream, followed by the buffer itself.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>length (4 bytes)&lt;/th>
&lt;th>nonce (12 bytes)&lt;/th>
&lt;th>ciphertext (length-12 bytes)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h3 id="52-crypto-structures">5.2 Crypto structures&lt;/h3>
&lt;p>Parquet file encryption algorithm is specified in a union of the following Thrift structures:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">AesGcmV1&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** AAD prefix **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">aad_prefix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Unique file identifier part of AAD suffix **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">aad_file_unique&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** In files encrypted with AAD prefix without storing it,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * readers must supply the prefix **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#204a87;font-weight:bold">bool&lt;/span> &lt;span style="color:#000">supply_aad_prefix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">AesGcmCtrV1&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** AAD prefix **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">aad_prefix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Unique file identifier part of AAD suffix **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">aad_file_unique&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** In files encrypted with AAD prefix without storing it,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * readers must supply the prefix **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#204a87;font-weight:bold">bool&lt;/span> &lt;span style="color:#000">supply_aad_prefix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">union&lt;/span> &lt;span style="color:#000">EncryptionAlgorithm&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">AesGcmV1&lt;/span> &lt;span style="color:#000">AES_GCM_V1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">AesGcmCtrV1&lt;/span> &lt;span style="color:#000">AES_GCM_CTR_V1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If a writer provides an AAD prefix, it will be used for enciphering the file and stored in the
&lt;code>aad_prefix&lt;/code> field. However, the writer can request Parquet not to store the prefix in the file. In
this case, the &lt;code>aad_prefix&lt;/code> field will not be set, and the &lt;code>supply_aad_prefix&lt;/code> field will be set
to &lt;em>true&lt;/em> to inform readers they must supply the AAD prefix for this file in order to be able to
decrypt it.&lt;/p>
&lt;p>The row group ordinal, required for AAD suffix calculation, is set in the RowGroup structure:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">RowGroup&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Row group ordinal in the file **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">7&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">i16&lt;/span> &lt;span style="color:#000">ordinal&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A &lt;code>crypto_metadata&lt;/code> field is set in each ColumnChunk in the encrypted columns. ColumnCryptoMetaData
is a union - the actual structure is chosen depending on whether the column is encrypted with the
footer encryption key, or with a column-specific key. For the latter, a key metadata can be specified.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">EncryptionWithFooterKey&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">EncryptionWithColumnKey&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Column path in schema **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">required&lt;/span> &lt;span style="color:#000">list&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;&lt;/span>&lt;span style="color:#000">string&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&lt;/span> &lt;span style="color:#000">path_in_schema&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Retrieval metadata of column encryption key **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">key_metadata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">union&lt;/span> &lt;span style="color:#000">ColumnCryptoMetaData&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">EncryptionWithFooterKey&lt;/span> &lt;span style="color:#000">ENCRYPTION_WITH_FOOTER_KEY&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">EncryptionWithColumnKey&lt;/span> &lt;span style="color:#000">ENCRYPTION_WITH_COLUMN_KEY&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">ColumnChunk&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Crypto metadata of encrypted columns **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">ColumnCryptoMetaData&lt;/span> &lt;span style="color:#000">crypto_metadata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="53-protection-of-sensitive-metadata">5.3 Protection of sensitive metadata&lt;/h3>
&lt;p>The Parquet file footer, and its nested structures, contain sensitive information - ranging
from a secret data (column statistics) to other information that can be exploited by an
attacker (e.g. schema, num_values, key_value_metadata, encoding
and crypto_metadata). This information is automatically protected when the footer and
secret columns are encrypted with the same key. In other cases - when column(s) and the
footer are encrypted with different keys; or column(s) are encrypted and the footer is not,
an extra measure is required to protect the column-specific information in the file footer.
In these cases, the &lt;code>ColumnMetaData&lt;/code> structures are Thrift-serialized separately and encrypted
with a column-specific key, thus protecting the column stats and
other metadata. The column metadata module is encrypted with the GCM cipher, serialized
according to the section 5.1 instructions and stored in an &lt;code>optional binary encrypted_column_metadata&lt;/code>
field in the &lt;code>ColumnChunk&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">ColumnChunk&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Column metadata for this chunk.. **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">ColumnMetaData&lt;/span> &lt;span style="color:#000">meta_data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">..&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Crypto metadata of encrypted columns **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">ColumnCryptoMetaData&lt;/span> &lt;span style="color:#000">crypto_metadata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Encrypted column metadata for this chunk **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">9&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">encrypted_column_metadata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="54-encrypted-footer-mode">5.4 Encrypted footer mode&lt;/h3>
&lt;p>In files with sensitive column data, a good security practice is to encrypt not only the
secret columns, but also the file footer metadata. This hides the file schema,
number of rows, key-value properties, column sort order, names of the encrypted columns
and metadata of the column encryption keys.&lt;/p>
&lt;p>The columns encrypted with the same key as the footer must leave the column metadata at the original
location, &lt;code>optional ColumnMetaData meta_data&lt;/code> in the &lt;code>ColumnChunk&lt;/code> structure.
This field is not set for columns encrypted with a column-specific key - instead, the &lt;code>ColumnMetaData&lt;/code>
is Thrift-serialized, encrypted with the column key and written to the &lt;code>encrypted_column_metadata&lt;/code>
field in the &lt;code>ColumnChunk&lt;/code> structure, as described in the section 5.3.&lt;/p>
&lt;p>A Thrift-serialized &lt;code>FileCryptoMetaData&lt;/code> structure is written before the encrypted footer.
It contains information on the file encryption algorithm and on the footer key metadata. Then
the combined length of this structure and of the encrypted footer is written as a 4-byte
little endian integer, followed by a final magic string, &amp;ldquo;PARE&amp;rdquo;. The same magic bytes are
written at the beginning of the file (offset 0). Parquet readers start file parsing by
reading and checking the magic string. Therefore, the encrypted footer mode uses a new
magic string (&amp;ldquo;PARE&amp;rdquo;) in order to instruct readers to look for a file crypto metadata
before the footer - and also to immediately inform legacy readers (expecting ‘PAR1’
bytes) that they can’t parse this file.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">/** Crypto metadata for files with encrypted footer **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">FileCryptoMetaData&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * Encryption algorithm. This field is only used for files
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * with encrypted footer. Files with plaintext footer store algorithm id
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * inside footer (FileMetaData structure).
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">required&lt;/span> &lt;span style="color:#000">EncryptionAlgorithm&lt;/span> &lt;span style="color:#000">encryption_algorithm&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/** Retrieval metadata of key used for encryption of footer,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * and (possibly) columns **/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">key_metadata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="File Layout - Encrypted footer" src="/images/FileLayoutEncryptionEF.png">&lt;/p>
&lt;h3 id="55-plaintext-footer-mode">5.5 Plaintext footer mode&lt;/h3>
&lt;p>This mode allows legacy Parquet versions (released before the encryption support) to access
unencrypted columns in encrypted files - at a price of leaving certain metadata fields
unprotected in these files.&lt;/p>
&lt;p>The plaintext footer mode can be useful during a transitional period in organizations where
some frameworks can&amp;rsquo;t be upgraded to a new Parquet library for a while. Data writers will
upgrade and run with a new Parquet version, producing encrypted files in this mode. Data
readers working with sensitive data will also upgrade to a new Parquet library. But other
readers that don&amp;rsquo;t need the sensitive columns, can continue working with an older Parquet
version. They will be able to access plaintext columns in encrypted files. A legacy reader,
trying to access a sensitive column data in an encrypted file with a plaintext footer, will
get an exception. More specifically, a Thrift parsing exception on an encrypted page header
structure. Again, using legacy Parquet readers for encrypted files is a temporary solution.&lt;/p>
&lt;p>In the plaintext footer mode, the &lt;code>optional ColumnMetaData meta_data&lt;/code> is set in the &lt;code>ColumnChunk&lt;/code>
structure for all columns, but is stripped of the statistics for the sensitive (encrypted)
columns. These statistics are available for new readers with the column key - they decrypt
the &lt;code>encrypted_column_metadata&lt;/code> field, described in the section 5.3, and parse it to get statistics
and all other column metadata values. The legacy readers are not aware of the encrypted metadata field;
they parse the regular (plaintext) field as usual. While they can&amp;rsquo;t read the data of encrypted
columns, they read their metadata to extract the offset and size of encrypted column data,
required for column chunk vectorization.&lt;/p>
&lt;p>The plaintext footer is signed in order to prevent tampering with the
&lt;code>FileMetaData&lt;/code> contents. The footer signing is done by encrypting the serialized &lt;code>FileMetaData&lt;/code>
structure with the
AES GCM algorithm - using a footer signing key, and an AAD constructed according to the instructions
of the section 4.4. Only the nonce and GCM tag are stored in the file – as a 28-byte
fixed-length array, written right after the footer itself. The ciphertext is not stored,
because it is not required for footer integrity verification by readers.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>nonce (12 bytes)&lt;/th>
&lt;th>tag (16 bytes)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>The plaintext footer mode sets the following fields in the the FileMetaData structure:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">struct&lt;/span> &lt;span style="color:#000">FileMetaData&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * Encryption algorithm. This field is set only in encrypted files
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * with plaintext footer. Files with encrypted footer store algorithm id
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * in FileCryptoMetaData structure.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">EncryptionAlgorithm&lt;/span> &lt;span style="color:#000">encryption_algorithm&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * Retrieval metadata of key used for signing the footer.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> * Used only in encrypted files with plaintext footer.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0000cf;font-weight:bold">9&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">:&lt;/span> &lt;span style="color:#000">optional&lt;/span> &lt;span style="color:#000">binary&lt;/span> &lt;span style="color:#000">footer_signing_key_metadata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>FileMetaData&lt;/code> structure is Thrift-serialized and written to the output stream.
The 28-byte footer signature is written after the plaintext footer, followed by a 4-byte little endian integer
that contains the combined length of the footer and its signature. A final magic string,
&amp;ldquo;PAR1&amp;rdquo;, is written at the end of the
file. The same magic string is written at the beginning of the file (offset 0). The magic bytes
for plaintext footer mode are ‘PAR1’ to allow legacy readers to read projections of the file
that do not include encrypted columns.&lt;/p>
&lt;p>&lt;img alt="File Layout - Encrypted footer" src="/images/FileLayoutEncryptionPF.png">&lt;/p>
&lt;h2 id="6-encryption-overhead">6. Encryption Overhead&lt;/h2>
&lt;p>The size overhead of Parquet modular encryption is negligible, since most of the encryption
operations are performed on pages (the minimal unit of Parquet data storage and compression).
The overhead order of magnitude is adding 1 byte per each ~30,000 bytes of original
data - calculated by comparing the page encryption overhead (nonce + tag + length = 32 bytes)
to the default page size (1 MB). This is a rough estimation, and can change with the encryption
algorithm (no 16-byte tag in AES_GCM_CTR_V1) and with page configuration or data encoding/compression.&lt;/p>
&lt;p>The throughput overhead of Parquet modular encryption depends on whether AES enciphering is
done in software or hardware. In both cases, performing encryption on full pages (~1MB buffers)
instead of on much smaller individual data values causes AES to work at its maximal speed.&lt;/p></description></item><item><title>Docs: License</title><link>/docs/asf/license/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/asf/license/</guid><description>
&lt;p>&lt;a href="https://www.apache.org/licenses/">License&lt;/a>&lt;/p></description></item><item><title>Docs: Modules</title><link>/docs/contribution-guidelines/modules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/contribution-guidelines/modules/</guid><description>
&lt;p>The &lt;a href="https://github.com/apache/parquet-format">parquet-format&lt;/a> project contains format specifications and Thrift definitions of metadata required to properly read Parquet files.&lt;/p>
&lt;p>The &lt;a href="https://github.com/apache/parquet-mr">parquet-mr&lt;/a> project contains multiple sub-modules, which implement the core components of reading and writing a nested, column-oriented data stream, map this core onto the parquet format, and provide Hadoop Input/Output Formats, Pig loaders, and other Java-based utilities for interacting with Parquet.&lt;/p>
&lt;p>The &lt;a href="https://arrow.apache.org/docs/cpp/parquet.html">parquet-cpp&lt;/a> project is a C++ library to read-write Parquet files. It is part of the &lt;a href="https://arrow.apache.org/">Apache Arrow&lt;/a> C++ implementation, with bindings to Python, R, Ruby and C/GLib.&lt;/p>
&lt;p>The &lt;a href="https://github.com/apache/arrow-rs/tree/master/parquet">parquet-rs&lt;/a> project is a Rust library to read-write Parquet files.&lt;/p>
&lt;p>The &lt;a href="https://github.com/Parquet/parquet-compatibility">parquet-compatibility&lt;/a> project (deprecated) contains compatibility tests that can be used to verify that implementations in different languages can read and write each other’s files. As of January 2022 compatibility tests only exist up to version 1.2.0.&lt;/p></description></item><item><title>Docs: Spark Summit 2020</title><link>/docs/learning-resources/presentations/spark-summit-2020/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/learning-resources/presentations/spark-summit-2020/</guid><description>
&lt;p>&lt;a href="https://www.slideshare.net/databricks/the-apache-spark-file-format-ecosystem">Slides&lt;/a>&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/auNAzC3AU18" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Building Parquet</title><link>/docs/contribution-guidelines/building/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/contribution-guidelines/building/</guid><description>
&lt;p>Building
Java resources can be build using &lt;code>mvn package&lt;/code>. The current stable version should always be available from Maven Central.&lt;/p>
&lt;p>C++ thrift resources can be generated via make.&lt;/p>
&lt;p>Thrift can be also code-genned into any other thrift-supported language.&lt;/p></description></item><item><title>Docs: Hadoop Summit 2014</title><link>/docs/learning-resources/presentations/hadoop-summit-2014/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/learning-resources/presentations/hadoop-summit-2014/</guid><description>
&lt;p>&lt;a href="https://www.slideshare.net/cloudera/hadoop-summit-36479635">Slides&lt;/a>&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/MZNjmfx4LMc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Motivation</title><link>/docs/overview/motivation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/overview/motivation/</guid><description>
&lt;p>We created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem.&lt;/p>
&lt;p>Parquet is built from the ground up with complex nested data structures in mind, and uses the &lt;a href="https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper">record shredding and assembly algorithm&lt;/a> described in the Dremel paper. We believe this approach is superior to simple flattening of nested name spaces.&lt;/p>
&lt;p>Parquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented.&lt;/p>
&lt;p>Parquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies.&lt;/p></description></item><item><title>Docs: Security</title><link>/docs/asf/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/asf/security/</guid><description>
&lt;p>&lt;a href="https://www.apache.org/security/">Security&lt;/a>&lt;/p></description></item><item><title>Docs: #CONF 2014</title><link>/docs/learning-resources/presentations/conf-2014-parquet-summit-twitter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/learning-resources/presentations/conf-2014-parquet-summit-twitter/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Qfp6Uv1UrA0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Contributing to Parquet</title><link>/docs/contribution-guidelines/contributing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/contribution-guidelines/contributing/</guid><description>
&lt;h2 id="pull-requests">Pull Requests&lt;/h2>
&lt;p>We prefer to receive contributions in the form of GitHub pull requests. Please send pull requests against the &lt;a href="https://github.com/apache/parquet-mr">github.com/apache/parquet-mr&lt;/a> repository. If you’ve previously forked Parquet from its old location, you will need to add a remote or update your origin remote to &lt;a href="https://github.com/apache/parquet-mr.git">https://github.com/apache/parquet-mr.git&lt;/a> Here are a few tips to get your contribution in:&lt;/p>
&lt;ol>
&lt;li>Break your work into small, single-purpose patches if possible. It’s much harder to merge in a large change with a lot of disjoint features.&lt;/li>
&lt;li>Create a JIRA for your patch on the &lt;a href="https://issues.apache.org/jira/browse/PARQUET">Parquet Project JIRA&lt;/a>.&lt;/li>
&lt;li>Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on forking a repo and sending a pull request. Prefix your pull request name with the JIRA name (ex: &lt;a href="https://github.com/apache/parquet-mr/pull/5">https://github.com/apache/parquet-mr/pull/5&lt;/a>).&lt;/li>
&lt;li>Make sure that your code passes the unit tests. You can run the tests with &lt;code>mvn test&lt;/code> in the root directory.&lt;/li>
&lt;li>Add new unit tests for your code.&lt;/li>
&lt;li>All Pull Requests are tested automatically on &lt;a href="https://github.com/apache/parquet-mr/actions">GitHub Actions&lt;/a>. &lt;a href="https://travis-ci.org/github/apache/parquet-mr">TravisCI&lt;/a> is also used to run the tests on ARM64 CPU architecture&lt;/li>
&lt;/ol>
&lt;p>If you’d like to report a bug but don’t have time to fix it, you can still post it to our &lt;a href="https://issues.apache.org/jira/browse/PARQUET">issue tracker&lt;/a>, or email the mailing list (&lt;a href="mailto:dev@parquet.apache.org">dev@parquet.apache.org&lt;/a>).&lt;/p>
&lt;h2 id="committers">Committers&lt;/h2>
&lt;p>Merging a pull request requires being a comitter on the project.&lt;/p>
&lt;p>How to merge a Pull request (have an apache and github-apache remote setup):&lt;/p>
&lt;pre>&lt;code>git remote add github-apache git@github.com:apache/parquet-mr.git
git remote add apache https://gitbox.apache.org/repos/asf?p=parquet-mr.git
&lt;/code>&lt;/pre>
&lt;p>run the following command&lt;/p>
&lt;pre>&lt;code>dev/merge_parquet_pr.py
&lt;/code>&lt;/pre>
&lt;p>example output:&lt;/p>
&lt;pre>&lt;code>Which pull request would you like to merge? (e.g. 34):
&lt;/code>&lt;/pre>
&lt;p>Type the pull request number (from &lt;a href="https://github.com/apache/parquet-mr/pulls">https://github.com/apache/parquet-mr/pulls&lt;/a>) and hit enter.&lt;/p>
&lt;pre>&lt;code>=== Pull Request #X ===
title Blah Blah Blah
source repo/branch
target master
url https://api.github.com/repos/apache/parquet-mr/pulls/X
Proceed with merging pull request #3? (y/n):
&lt;/code>&lt;/pre>
&lt;p>If this looks good, type &lt;code>y&lt;/code> and hit enter.&lt;/p>
&lt;pre>&lt;code>From gitbox.apache.org:/repos/asf/parquet-mr.git
* [new branch] master -&amp;gt; PR_TOOL_MERGE_PR_3_MASTER
Switched to branch 'PR_TOOL_MERGE_PR_3_MASTER'
Merge complete (local ref PR_TOOL_MERGE_PR_3_MASTER). Push to apache? (y/n):
&lt;/code>&lt;/pre>
&lt;p>A local branch with the merge has been created. Type &lt;code>y&lt;/code> and hit enter to push it to apache master&lt;/p>
&lt;pre>&lt;code>Counting objects: 67, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (26/26), done.
Writing objects: 100% (36/36), 5.32 KiB, done.
Total 36 (delta 17), reused 0 (delta 0)
To gitbox.apache.org:/repos/asf/parquet-mr.git
b767ac4..485658a PR_TOOL_MERGE_PR_X_MASTER -&amp;gt; master
Restoring head pointer to b767ac4e
Note: checking out 'b767ac4e'.
You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.
If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:
git checkout -b new_branch_name
HEAD is now at b767ac4... Update README.md
Deleting local branch PR_TOOL_MERGE_PR_X
Deleting local branch PR_TOOL_MERGE_PR_X_MASTER
Pull request #X merged!
Merge hash: 485658a5
Would you like to pick 485658a5 into another branch? (y/n):
&lt;/code>&lt;/pre>
&lt;p>For now just say &lt;code>n&lt;/code> as we have 1 branch&lt;/p>
&lt;h2 id="website">Website&lt;/h2>
&lt;h3 id="release-documentation">Release Documentation&lt;/h3>
&lt;p>To create documentation for a new release of &lt;code>parquet-format&lt;/code> create a new &lt;releaseNumber>.md file under &lt;code>content/en/blog/parquet-format&lt;/code>. Please see existing files in that directory as an example.&lt;/p>
&lt;p>To create documentation for a new release of &lt;code>parquet-mr&lt;/code> create a new &lt;releaseNumber>.md file under &lt;code>content/en/blog/parquet-mr&lt;/code>. Please see existing files in that directory as an example.&lt;/p>
&lt;h3 id="website-development-and-deployment">Website development and deployment&lt;/h3>
&lt;h4 id="staging">Staging&lt;/h4>
&lt;p>To make a change to the &lt;code>staging&lt;/code> version of the website:&lt;/p>
&lt;ol>
&lt;li>Make a PR against the &lt;code>staging&lt;/code> branch in the repository&lt;/li>
&lt;li>Once the PR is merged, the &lt;code>Build and Deploy Parquet Site&lt;/code>
job in the &lt;a href="https://github.com/apache/parquet-site/blob/staging/.github/workflows/deploy.yml">deployment workflow&lt;/a> will be run, populating the &lt;code>asf-staging&lt;/code> branch on this repo with the necessary files.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Do not directly edit the &lt;code>asf-staging&lt;/code> branch of this repo&lt;/strong>&lt;/p>
&lt;h4 id="production">Production&lt;/h4>
&lt;p>To make a change to the &lt;code>production&lt;/code> version of the website:&lt;/p>
&lt;ol>
&lt;li>Make a PR against the &lt;code>production&lt;/code> branch in the repository&lt;/li>
&lt;li>Once the PR is merged, the &lt;code>Build and Deploy Parquet Site&lt;/code>
job in the &lt;a href="https://github.com/apache/parquet-site/blob/production/.github/workflows/deploy.yml">deployment workflow&lt;/a> will be run, populating the &lt;code>asf-site&lt;/code> branch on this repo with the necessary files.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Do not directly edit the &lt;code>asf-site&lt;/code> branch of this repo&lt;/strong>&lt;/p></description></item><item><title>Docs: Sponsor</title><link>/docs/asf/sponsor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/asf/sponsor/</guid><description>
&lt;p>&lt;a href="https://www.apache.org/foundation/thanks.html">Sponsor&lt;/a>&lt;/p></description></item><item><title>Docs: Donate</title><link>/docs/asf/donate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/asf/donate/</guid><description>
&lt;p>&lt;a href="https://www.apache.org/foundation/sponsorship.html">Donate&lt;/a>&lt;/p></description></item><item><title>Docs: Releasing Parquet</title><link>/docs/contribution-guidelines/releasing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/contribution-guidelines/releasing/</guid><description>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;p>You will need: * PGP code signing keys, published in &lt;a href="https://downloads.apache.org/parquet/KEYS">KEYS&lt;/a> * Permission to stage artifacts in Nexus&lt;/p>
&lt;p>Make sure you have permission to deploy Parquet artifacts to Nexus by pushing a snapshot:&lt;/p>
&lt;pre>&lt;code>mvn deploy
&lt;/code>&lt;/pre>
&lt;p>If you have problems, read the &lt;a href="https://www.apache.org/dev/publishing-maven-artifacts.html">publishing Maven artifacts documentation&lt;/a>&lt;/p>
&lt;h3 id="release-process">Release process&lt;/h3>
&lt;p>Parquet uses the maven-release-plugin to tag a release and push binary artifacts to staging in Nexus. Once maven completes the release, the offical source tarball is built from the tag.&lt;/p>
&lt;p>Before you start the release process:&lt;/p>
&lt;ol>
&lt;li>Verify that the release is finished (no planned JIRAs are pending and all patches are cherry-picked to the release branch)&lt;/li>
&lt;li>Resolve all associated JIRAs with correct target version and create the next unreleased version in the JIRA project&lt;/li>
&lt;li>Build and test the project&lt;/li>
&lt;li>Create a new branch for the release if this is a new minor version. For example, if the new minor version is 1.13.0, create a new branch &lt;code>parquet-1.13.x&lt;/code>&lt;/li>
&lt;li>Update the change log
&lt;ul>
&lt;li>Go to the release notes for the release in JIRA&lt;/li>
&lt;li>Copy the HTML and convert it to markdown with an &lt;a href="https://domchristie.github.io/turndown/">online converter&lt;/a>&lt;/li>
&lt;li>Add the content to CHANGES.md and update formatting&lt;/li>
&lt;li>Commit the update to CHANGES.md and make sure it is committed to both release and master branches&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h4 id="1-run-the-prepare-script">1. Run the prepare script&lt;/h4>
&lt;pre>&lt;code>dev/prepare-release.sh &amp;lt;version&amp;gt; &amp;lt;rc-number&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This runs maven’s release prepare with a consistent tag name. After this step, the release tag will exist in the git repository.&lt;/p>
&lt;p>If this step fails, you can roll back the changes by running these commands.&lt;/p>
&lt;pre>&lt;code>find ./ -type f -name '*.releaseBackup' -exec rm {} \;
find ./ -type f -name 'pom.xml' -exec git checkout {} \;
&lt;/code>&lt;/pre>
&lt;h4 id="2-run-releaseperform-to-stage-binaries">2. Run release:perform to stage binaries&lt;/h4>
&lt;pre>&lt;code>mvn release:perform
&lt;/code>&lt;/pre>
&lt;p>This uploads binary artifacts for the release tag to &lt;a href="https://repository.apache.org/">Nexus&lt;/a>.&lt;/p>
&lt;h4 id="3-in-nexus-close-the-staging-repository">3. In Nexus, close the staging repository&lt;/h4>
&lt;p>Closing a staging repository makes the binaries available in &lt;a href="https://repository.apache.org/content/groups/staging/org/apache/parquet/">staging&lt;/a>, but does not publish them.&lt;/p>
&lt;ol>
&lt;li>Go to &lt;a href="https://repository.apache.org/">Nexus&lt;/a>.&lt;/li>
&lt;li>In the menu on the left, choose “Staging Repositories”.&lt;/li>
&lt;li>Select the Parquet repository.&lt;/li>
&lt;li>At the top, click “Close” and follow the instructions. For the comment use “Apache Parquet [Format] ”.&lt;/li>
&lt;/ol>
&lt;h4 id="4-run-the-source-tarball-script">4. Run the source tarball script&lt;/h4>
&lt;pre>&lt;code>dev/source-release.sh &amp;lt;version&amp;gt; &amp;lt;rc-number&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This script builds the source tarball from the release tag’s SHA1, signs it, and uploads the necessary files with SVN.&lt;/p>
&lt;p>The source release is pushed to &lt;a href="https://dist.apache.org/repos/dist/dev/parquet/">https://dist.apache.org/repos/dist/dev/parquet/&lt;/a>&lt;/p>
&lt;p>The last message from the script is the release commit’s SHA1 hash and URL for the VOTE e-mail.&lt;/p>
&lt;h4 id="5-send-a-vote-e-mail-to-devparquetapacheorgmailtodevparquetapacheorg">5. Send a VOTE e-mail to &lt;a href="mailto:dev@parquet.apache.org">dev@parquet.apache.org&lt;/a>&lt;/h4>
&lt;p>Here is a template you can use. Make sure everything applies to your release.&lt;/p>
&lt;pre>&lt;code>Subject: [VOTE] Release Apache Parquet &amp;lt;VERSION&amp;gt; RC&amp;lt;NUM&amp;gt;
Hi everyone,
I propose the following RC to be released as official Apache Parquet &amp;lt;VERSION&amp;gt; release.
The commit id is &amp;lt;SHA1&amp;gt;
* This corresponds to the tag: apache-parquet-&amp;lt;VERSION&amp;gt;-rc&amp;lt;NUM&amp;gt;
* https://github.com/apache/parquet-mr/tree/&amp;lt;SHA1&amp;gt;
The release tarball, signature, and checksums are here:
* https://dist.apache.org/repos/dist/dev/parquet/&amp;lt;PATH&amp;gt;
You can find the KEYS file here:
* https://downloads.apache.org/parquet/KEYS
Binary artifacts are staged in Nexus here:
* https://repository.apache.org/content/groups/staging/org/apache/parquet/
This release includes important changes that I should have summarized here, but I'm lazy.
Please download, verify, and test.
Please vote in the next 72 hours.
[ ] +1 Release this as Apache Parquet &amp;lt;VERSION&amp;gt;
[ ] +0
[ ] -1 Do not release this because...
&lt;/code>&lt;/pre>
&lt;h3 id="publishing-after-the-vote-passes">Publishing after the vote passes&lt;/h3>
&lt;p>After a release candidate passes a vote, the candidate needs to be published as the final release.&lt;/p>
&lt;h4 id="1-tag-final-release-and-set-development-version">1. Tag final release and set development version&lt;/h4>
&lt;pre>&lt;code>dev/finalize-release &amp;lt;release-version&amp;gt; &amp;lt;rc-num&amp;gt; &amp;lt;new-development-version-without-SNAPSHOT-suffix&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This will add the final release tag to the RC tag and sets the new development version in the pom files. If everything is fine push the changes and the new tag to github: &lt;code>git push --follow-tags&lt;/code>&lt;/p>
&lt;h4 id="2-release-the-binary-repository-in-nexus">2. Release the binary repository in Nexus&lt;/h4>
&lt;p>Releasing a binary repository publishes the binaries to &lt;a href="https://repository.apache.org/content/groups/public/org/apache/parquet/">public&lt;/a>.&lt;/p>
&lt;ol>
&lt;li>Go to &lt;a href="https://repository.apache.org/">Nexus&lt;/a>.&lt;/li>
&lt;li>In the menu on the left, choose “Staging Repositories”.&lt;/li>
&lt;li>Select the Parquet repository.&lt;/li>
&lt;li>At the top, click Release and follow the instructions. For the comment use “Apache Parquet [Format] ”.&lt;/li>
&lt;/ol>
&lt;h4 id="3-copy-the-release-artifacts-in-svn-into-releases">3. Copy the release artifacts in SVN into releases&lt;/h4>
&lt;p>First, check out the candidates and releases locations in SVN:&lt;/p>
&lt;pre>&lt;code>mkdir parquet
cd parquet
svn co https://dist.apache.org/repos/dist/dev/parquet candidates
svn co https://dist.apache.org/repos/dist/release/parquet releases
&lt;/code>&lt;/pre>
&lt;p>Next, copy the directory for the release candidate the passed from candidates to releases and rename it; remove the “-rcN” part of the directory name.&lt;/p>
&lt;pre>&lt;code>cp -r candidates/apache-parquet-&amp;lt;VERSION&amp;gt;-rcN/ releases/apache-parquet-&amp;lt;VERSION&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Then add and commit the release artifacts:&lt;/p>
&lt;pre>&lt;code>cd releases
svn add apache-parquet-&amp;lt;version&amp;gt;
svn ci -m &amp;quot;Parquet: Add release &amp;lt;VERSION&amp;gt;&amp;quot;
&lt;/code>&lt;/pre>
&lt;h4 id="4-update-parquetapacheorg">4. Update parquet.apache.org&lt;/h4>
&lt;p>Update the downloads page on parquet.apache.org. Instructions for updating the site are on the &lt;a href="http://parquet.apache.org/docs/contribution-guidelines/contributing/">contribution page&lt;/a>.&lt;/p>
&lt;h4 id="5-send-an-announce-e-mail-to-announceapacheorgmailtoannounceapacheorg-and-the-dev-list">5. Send an ANNOUNCE e-mail to &lt;a href="mailto:announce@apache.org">announce@apache.org&lt;/a> and the dev list&lt;/h4>
&lt;pre>&lt;code>[ANNOUNCE] Apache Parquet release &amp;lt;VERSION&amp;gt;
I'm please to announce the release of Parquet &amp;lt;VERSION&amp;gt;!
Parquet is a general-purpose columnar file format for nested data. It uses
space-efficient encodings and a compressed and splittable structure for
processing frameworks like Hadoop.
Changes are listed at: https://github.com/apache/parquet-mr/blob/apache-parquet-&amp;lt;VERSION&amp;gt;/CHANGES.md
This release can be downloaded from: https://parquet.apache.org/downloads/
Java artifacts are available from Maven Central.
Thanks to everyone for contributing!
&lt;/code>&lt;/pre></description></item><item><title>Docs: Strata 2013</title><link>/docs/learning-resources/presentations/strata-2013/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/learning-resources/presentations/strata-2013/</guid><description>
&lt;p>&lt;a href="https://www.slideshare.net/julienledem/parquet-stratany-hadoopworld2013">Slides&lt;/a>&lt;/p></description></item><item><title>Docs: Configurations</title><link>/docs/file-format/configurations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/configurations/</guid><description>
&lt;h3 id="row-group-size">Row Group Size&lt;/h3>
&lt;p>Larger row groups allow for larger column chunks which makes it
possible to do larger sequential IO. Larger groups also require more buffering in
the write path (or a two pass write). We recommend large row groups (512MB - 1GB).
Since an entire row group might need to be read, we want it to completely fit on
one HDFS block. Therefore, HDFS block sizes should also be set to be larger. An
optimized read setup would be: 1GB row groups, 1GB HDFS block size, 1 HDFS block
per HDFS file.&lt;/p>
&lt;h3 id="data-page--size">Data Page Size&lt;/h3>
&lt;p>Data pages should be considered indivisible so smaller data pages
allow for more fine grained reading (e.g. single row lookup). Larger page sizes
incur less space overhead (less page headers) and potentially less parsing overhead
(processing headers). Note: for sequential scans, it is not expected to read a page
at a time; this is not the IO chunk. We recommend 8KB for page sizes.&lt;/p></description></item><item><title>Docs: Events</title><link>/docs/asf/events/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/asf/events/</guid><description>
&lt;p>&lt;a href="https://apachecon.com/?ref=parquet.apache.org">Events&lt;/a>&lt;/p></description></item><item><title>Docs: Extensibility</title><link>/docs/file-format/extensibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/extensibility/</guid><description>
&lt;p>There are many places in the format for compatible extensions:&lt;/p>
&lt;ul>
&lt;li>File Version: The file metadata contains a version.&lt;/li>
&lt;li>Encodings: Encodings are specified by enum and more can be added in the future.&lt;/li>
&lt;li>Page types: Additional page types can be added and safely skipped.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Logical Types</title><link>/docs/file-format/types/logicaltypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/types/logicaltypes/</guid><description>
&lt;p>Logical types are used to extend the types that parquet can be used to store,
by specifying how the primitive types should be interpreted. This keeps the set
of primitive types to a minimum and reuses parquet&amp;rsquo;s efficient encodings. For
example, strings are stored as byte arrays (binary) with a UTF8 annotation.
These annotations define how to further decode and interpret the data.
Annotations are stored as &lt;code>LogicalType&lt;/code> fields in the file metadata and are
documented in LogicalTypes.md.&lt;/p></description></item><item><title>Docs: Metadata</title><link>/docs/file-format/metadata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/metadata/</guid><description>
&lt;p>There are three types of metadata: file metadata, column (chunk) metadata and page
header metadata. All thrift structures are serialized using the TCompactProtocol.&lt;/p>
&lt;p>&lt;img alt="File Layout" src="/images/FileFormat.gif">&lt;/p></description></item><item><title>Docs: Nested Encoding</title><link>/docs/file-format/nestedencoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/nestedencoding/</guid><description>
&lt;p>To encode nested columns, Parquet uses the Dremel encoding with definition and
repetition levels. Definition levels specify how many optional fields in the
path for the column are defined. Repetition levels specify at what repeated field
in the path has the value repeated. The max definition and repetition levels can
be computed from the schema (i.e. how much nesting there is). This defines the
maximum number of bits required to store the levels (levels are defined for all
values in the column).&lt;/p>
&lt;p>Two encodings for the levels are supported BIT_PACKED and RLE. Only RLE is now used as it supersedes BIT_PACKED.&lt;/p></description></item><item><title>Docs: Bloom Filter</title><link>/docs/file-format/bloomfilter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/bloomfilter/</guid><description>
&lt;h3 id="problem-statement">Problem statement&lt;/h3>
&lt;p>In their current format, column statistics and dictionaries can be used for predicate
pushdown. Statistics include minimum and maximum value, which can be used to filter out
values not in the range. Dictionaries are more specific, and readers can filter out values
that are between min and max but not in the dictionary. However, when there are too many
distinct values, writers sometimes choose not to add dictionaries because of the extra
space they occupy. This leaves columns with large cardinalities and widely separated min
and max without support for predicate pushdown.&lt;/p>
&lt;p>A &lt;a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter&lt;/a> is a compact data structure that
overapproximates a set. It can respond to membership queries with either &amp;ldquo;definitely no&amp;rdquo; or
&amp;ldquo;probably yes&amp;rdquo;, where the probability of false positives is configured when the filter is
initialized. Bloom filters do not have false negatives.&lt;/p>
&lt;p>Because Bloom filters are small compared to dictionaries, they can be used for predicate
pushdown even in columns with high cardinality and when space is at a premium.&lt;/p>
&lt;h3 id="goal">Goal&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Enable predicate pushdown for high-cardinality columns while using less space than
dictionaries.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Induce no additional I/O overhead when executing queries on columns without Bloom
filters attached or when executing non-selective queries.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="technical-approach">Technical Approach&lt;/h3>
&lt;p>The section describes split block Bloom filters, which is the first
(and, at time of writing, only) Bloom filter representation supported
in Parquet.&lt;/p>
&lt;p>First we will describe a &amp;ldquo;block&amp;rdquo;. This is the main component split
block Bloom filters are composed of.&lt;/p>
&lt;p>Each block is 256 bits, broken up into eight contiguous &amp;ldquo;words&amp;rdquo;, each
consisting of 32 bits. Each word is thought of as an array of bits;
each bit is either &amp;ldquo;set&amp;rdquo; or &amp;ldquo;not set&amp;rdquo;.&lt;/p>
&lt;p>When initialized, a block is &amp;ldquo;empty&amp;rdquo;, which means each of the eight
component words has no bits set. In addition to initialization, a
block supports two other operations: &lt;code>block_insert&lt;/code> and
&lt;code>block_check&lt;/code>. Both take a single unsigned 32-bit integer as input;
&lt;code>block_insert&lt;/code> returns no value, but modifies the block, while
&lt;code>block_check&lt;/code> returns a boolean. The semantics of &lt;code>block_check&lt;/code> are
that it must return &lt;code>true&lt;/code> if &lt;code>block_insert&lt;/code> was previously called on
the block with the same argument, and otherwise it returns &lt;code>false&lt;/code>
with high probability. For more details of the probability, see below.&lt;/p>
&lt;p>The operations &lt;code>block_insert&lt;/code> and &lt;code>block_check&lt;/code> depend on some
auxiliary artifacts. First, there is a sequence of eight odd unsigned
32-bit integer constants called the &lt;code>salt&lt;/code>. Second, there is a method
called &lt;code>mask&lt;/code> that takes as its argument a single unsigned 32-bit
integer and returns a block in which each word has exactly one bit
set.&lt;/p>
&lt;pre tabindex="0">&lt;code>unsigned int32 salt[8] = {0x47b6137bU, 0x44974d91U, 0x8824ad5bU,
0xa2b7289dU, 0x705495c7U, 0x2df1424bU,
0x9efc4947U, 0x5c6bfb31U}
block mask(unsigned int32 x) {
block result
for i in [0..7] {
unsigned int32 y = x * salt[i]
result.getWord(i).setBit(y &amp;gt;&amp;gt; 27)
}
return result
}
&lt;/code>&lt;/pre>&lt;p>Since there are eight words in the block and eight integers in the
salt, there is a correspondence between them. To set a bit in the nth
word of the block, &lt;code>mask&lt;/code> first multiplies its argument by the nth
integer in the &lt;code>salt&lt;/code>, keeping only the least significant 32 bits of
the 64-bit product, then divides that 32-bit unsigned integer by 2 to
the 27th power, denoted above using the C language&amp;rsquo;s right shift
operator &amp;ldquo;&lt;code>&amp;gt;&amp;gt;&lt;/code>&amp;rdquo;. The resulting integer is between 0 and 31,
inclusive. That integer is the bit that gets set in the word in the
block.&lt;/p>
&lt;p>From the &lt;code>mask&lt;/code> operation, &lt;code>block_insert&lt;/code> is defined as setting every
bit in the block that was also set in the result from mask. Similarly,
&lt;code>block_check&lt;/code> returns &lt;code>true&lt;/code> when every bit that is set in the result
of &lt;code>mask&lt;/code> is also set in the block.&lt;/p>
&lt;pre tabindex="0">&lt;code>void block_insert(block b, unsigned int32 x) {
block masked = mask(x)
for i in [0..7] {
for j in [0..31] {
if (masked.getWord(i).isSet(j)) {
b.getWord(i).setBit(j)
}
}
}
}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>boolean block_check(block b, unsigned int32 x) {
block masked = mask(x)
for i in [0..7] {
for j in [0..31] {
if (masked.getWord(i).isSet(j)) {
if (not b.getWord(i).setBit(j)) {
return false
}
}
}
}
return true
}
&lt;/code>&lt;/pre>&lt;p>The reader will note that a block, as defined here, is actually a
special kind of Bloom filter. Specifically it is a &amp;ldquo;split&amp;rdquo; Bloom
filter, as described in section 2.1 of &lt;a href="https://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf">Network Applications of Bloom
Filters: A
Survey&lt;/a>. The
use of multiplication by an odd constant and then shifting right is a
method of hashing integers as described in section 2.2 of
Dietzfelbinger et al.&amp;rsquo;s &lt;a href="http://hjemmesider.diku.dk/~jyrki/Paper/CP-11.4.1997.pdf">A reliable randomized algorithm for the
closest-pair
problem&lt;/a>.&lt;/p>
&lt;p>This closes the definition of a block and the operations on it.&lt;/p>
&lt;p>Now that a block is defined, we can describe Parquet&amp;rsquo;s split block
Bloom filters. A split block Bloom filter (henceforth &amp;ldquo;SBBF&amp;rdquo;) is
composed of &lt;code>z&lt;/code> blocks, where &lt;code>z&lt;/code> is greater than or equal to one and
less than 2 to the 31st power. When an SBBF is initialized, each block
in it is initialized, which means each bit in each word in each block
in the SBBF is unset.&lt;/p>
&lt;p>In addition to initialization, an SBBF supports an operation called
&lt;code>filter_insert&lt;/code> and one called &lt;code>filter_check&lt;/code>. Each takes as an
argument a 64-bit unsigned integer; &lt;code>filter_check&lt;/code> returns a boolean
and &lt;code>filter_insert&lt;/code> does not return a value, but does modify the SBBF.&lt;/p>
&lt;p>The &lt;code>filter_insert&lt;/code> operation first uses the most significant 32 bits
of its argument to select a block to operate on. Call the argument
&amp;ldquo;&lt;code>h&lt;/code>&amp;rdquo;, and recall the use of &amp;ldquo;&lt;code>z&lt;/code>&amp;rdquo; to mean the number of blocks. Then
a block number &lt;code>i&lt;/code> between &lt;code>0&lt;/code> and &lt;code>z-1&lt;/code> (inclusive) to operate on is
chosen as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">unsigned&lt;/span> &lt;span style="color:#000">int64&lt;/span> &lt;span style="color:#000">h_top_bits&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">h&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">32&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">unsigned&lt;/span> &lt;span style="color:#000">int64&lt;/span> &lt;span style="color:#000">z_as_64_bit&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">z&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">unsigned&lt;/span> &lt;span style="color:#000">int32&lt;/span> &lt;span style="color:#000">i&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">h_top_bits&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span> &lt;span style="color:#000">z_as_64_bit&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">32&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The first line extracts the most significant 32 bits from &lt;code>h&lt;/code> and
assignes them to a 64-bit unsigned integer. The second line is
simpler: it just sets an unsigned 64-bit value to the same value as
the 32-bit unsigned value &lt;code>z&lt;/code>. The purpose of having both &lt;code>h_top_bits&lt;/code>
and &lt;code>z_as_64_bit&lt;/code> be 64-bit values is so that their product is a
64-bit value. That product is taken in the third line, and then the
most significant 32 bits are extracted into the value &lt;code>i&lt;/code>, which is
the index of the block that will be operated on.&lt;/p>
&lt;p>After this process to select &lt;code>i&lt;/code>, &lt;code>filter_insert&lt;/code> uses the least
significant 32 bits of &lt;code>h&lt;/code> as the argument to &lt;code>block_insert&lt;/code> called on
block &lt;code>i&lt;/code>.&lt;/p>
&lt;p>The technique for converting the most significant 32 bits to an
integer between &lt;code>0&lt;/code> and &lt;code>z-1&lt;/code> (inclusive) avoids using the modulo
operation, which is often very slow. This trick can be found in
&lt;a href="https://domino.research.ibm.com/library/cyberdig.nsf/papers/DF54E3545C82E8A585257222006FD9A2/$File/rc24100.pdf">Kenneth A. Ross&amp;rsquo;s 2006 IBM research report, &amp;ldquo;Efficient Hash Probes on
Modern Processors&amp;rdquo;&lt;/a>&lt;/p>
&lt;p>The &lt;code>filter_check&lt;/code> operation uses the same method as &lt;code>filter_insert&lt;/code>
to select a block to operate on, then uses the least significant 32
bits of its argument as an argument to &lt;code>block_check&lt;/code> called on that
block, returning the result.&lt;/p>
&lt;p>In the pseudocode below, the modulus operator is represented with the C
language&amp;rsquo;s &amp;ldquo;&lt;code>%&lt;/code>&amp;rdquo; operator. The &amp;ldquo;&lt;code>&amp;gt;&amp;gt;&lt;/code>&amp;rdquo; operator is used to denote the
conversion of an unsigned 64-bit integer to an unsigned 32-bit integer
containing only the most significant 32 bits, and C&amp;rsquo;s cast operator
&amp;ldquo;&lt;code>(unsigned int32)&lt;/code>&amp;rdquo; is used to denote the conversion of an unsigned
64-bit integer to an unsigned 32-bit integer containing only the least
significant 32 bits.&lt;/p>
&lt;pre tabindex="0">&lt;code>void filter_insert(SBBF filter, unsigned int64 x) {
unsigned int64 i = ((x &amp;gt;&amp;gt; 32) * filter.numberOfBlocks()) &amp;gt;&amp;gt; 32;
block b = filter.getBlock(i);
block_insert(b, (unsigned int32)x)
}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>boolean filter_check(SBBF filter, unsigned int64 x) {
unsigned int64 i = ((x &amp;gt;&amp;gt; 32) * filter.numberOfBlocks()) &amp;gt;&amp;gt; 32;
block b = filter.getBlock(i);
return block_check(b, (unsigned int32)x)
}
&lt;/code>&lt;/pre>&lt;p>The use of blocks is from Putze et al.&amp;rsquo;s &lt;a href="http://algo2.iti.kit.edu/documents/cacheefficientbloomfilters-jea.pdf">Cache-, Hash- and
Space-Efficient Bloom
filters&lt;/a>&lt;/p>
&lt;p>To use an SBBF for values of arbitrary Parquet types, we apply a hash
function to that value - at the time of writing,
&lt;a href="https://cyan4973.github.io/xxHash/">xxHash&lt;/a>, using the function XXH64
with a seed of 0 and &lt;a href="https://github.com/Cyan4973/xxHash/blob/v0.7.0/doc/xxhash_spec.md">following the specification version
0.1.1&lt;/a>.&lt;/p>
&lt;h4 id="sizing-an-sbbf">Sizing an SBBF&lt;/h4>
&lt;p>The &lt;code>check&lt;/code> operation in SBBFs can return &lt;code>true&lt;/code> for an argument that
was never inserted into the SBBF. These are called &amp;ldquo;false
positives&amp;rdquo;. The &amp;ldquo;false positive probabilty&amp;rdquo; is the probability that
any given hash value that was never &lt;code>insert&lt;/code>ed into the SBBF will
cause &lt;code>check&lt;/code> to return &lt;code>true&lt;/code> (a false positive). There is not a
simple closed-form calculation of this probability, but here is an
example:&lt;/p>
&lt;p>A filter that uses 1024 blocks and has had 26,214 hash values
&lt;code>insert&lt;/code>ed will have a false positive probabilty of around 1.26%. Each
of those 1024 blocks occupies 256 bits of space, so the total space
usage is 262,144. That means that the ratio of bits of space to hash
values is 10-to-1. Adding more hash values increases the denominator
and lowers the ratio, which increases the false positive
probability. For instance, inserting twice as many hash values
(52,428) decreases the ratio of bits of space per hash value inserted
to 5-to-1 and increases the false positive probability to
18%. Inserting half as many hash values (13,107) increases the ratio
of bits of space per hash value inserted to 20-to-1 and decreases the
false positive probability to 0.04%.&lt;/p>
&lt;p>Here are some sample values of the ratios needed to achieve certain
false positive rates:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Bits of space per &lt;code>insert&lt;/code>&lt;/th>
&lt;th>False positive probability&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>6.0&lt;/td>
&lt;td>10 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.5&lt;/td>
&lt;td>1 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16.9&lt;/td>
&lt;td>0.1 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26.4&lt;/td>
&lt;td>0.01 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>41&lt;/td>
&lt;td>0.001 %&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="file-format">File Format&lt;/h4>
&lt;p>Each multi-block Bloom filter is required to work for only one column chunk. The data of a multi-block
bloom filter consists of the bloom filter header followed by the bloom filter bitset. The bloom filter
header encodes the size of the bloom filter bit set in bytes that is used to read the bitset.&lt;/p>
&lt;p>Here are the Bloom filter definitions in thrift:&lt;/p>
&lt;pre tabindex="0">&lt;code>/** Block-based algorithm type annotation. **/
struct SplitBlockAlgorithm {}
/** The algorithm used in Bloom filter. **/
union BloomFilterAlgorithm {
/** Block-based Bloom filter. **/
1: SplitBlockAlgorithm BLOCK;
}
/** Hash strategy type annotation. xxHash is an extremely fast non-cryptographic hash
* algorithm. It uses 64 bits version of xxHash.
**/
struct XxHash {}
/**
* The hash function used in Bloom filter. This function takes the hash of a column value
* using plain encoding.
**/
union BloomFilterHash {
/** xxHash Strategy. **/
1: XxHash XXHASH;
}
/**
* The compression used in the Bloom filter.
**/
struct Uncompressed {}
union BloomFilterCompression {
1: Uncompressed UNCOMPRESSED;
}
/**
* Bloom filter header is stored at beginning of Bloom filter data of each column
* and followed by its bitset.
**/
struct BloomFilterPageHeader {
/** The size of bitset in bytes **/
1: required i32 numBytes;
/** The algorithm for setting bits. **/
2: required BloomFilterAlgorithm algorithm;
/** The hash function used for Bloom filter. **/
3: required BloomFilterHash hash;
/** The compression used in the Bloom filter **/
4: required BloomFilterCompression compression;
}
struct ColumnMetaData {
...
/** Byte offset from beginning of file to Bloom filter data. **/
14: optional i64 bloom_filter_offset;
}
&lt;/code>&lt;/pre>&lt;p>The Bloom filters are grouped by row group and with data for each column in the same order as the file schema.
The Bloom filter data can be stored before the page indexes after all row groups. The file layout looks like:
&lt;img alt="File Layout - Bloom filter footer" src="/images/FileLayoutBloomFilter2.png">&lt;/p>
&lt;p>Or it can be stored between row groups, the file layout looks like:
&lt;img alt="File Layout - Bloom filter footer" src="/images/FileLayoutBloomFilter1.png">&lt;/p>
&lt;h4 id="encryption">Encryption&lt;/h4>
&lt;p>In the case of columns with sensitive data, the Bloom filter exposes a subset of sensitive
information such as the presence of value. Therefore the Bloom filter of columns with sensitive
data should be encrypted with the column key, and the Bloom filter of other (not sensitive) columns
do not need to be encrypted.&lt;/p>
&lt;p>Bloom filters have two serializable modules - the PageHeader thrift structure (with its internal
fields, including the BloomFilterPageHeader &lt;code>bloom_filter_page_header&lt;/code>), and the Bitset. The header
structure is serialized by Thrift, and written to file output stream; it is followed by the
serialized Bitset.&lt;/p>
&lt;p>For Bloom filters in sensitive columns, each of the two modules will be encrypted after
serialization, and then written to the file. The encryption will be performed using the AES GCM
cipher, with the same column key, but with different AAD module types - &amp;ldquo;BloomFilter Header&amp;rdquo; (8)
and &amp;ldquo;BloomFilter Bitset&amp;rdquo; (9). The length of the encrypted buffer is written before the buffer, as
described in the Parquet encryption specification.&lt;/p></description></item><item><title>Docs: Checksumming</title><link>/docs/file-format/data-pages/checksumming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/data-pages/checksumming/</guid><description>
&lt;p>Pages of all kinds can be individually checksummed. This allows disabling of checksums
at the HDFS file level, to better support single row lookups. Checksums are calculated
using the standard CRC32 algorithm - as used in e.g. GZip - on the serialized binary
representation of a page (not including the page header itself).&lt;/p></description></item><item><title>Docs: Column Chunks</title><link>/docs/file-format/data-pages/columnchunks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/data-pages/columnchunks/</guid><description>
&lt;p>Column chunks are composed of pages written back to back. The pages share a common
header and readers can skip over pages they are not interested in. The data for the
page follows the header and can be compressed and/or encoded. The compression and
encoding is specified in the page metadata.&lt;/p>
&lt;p>A column chunk might be partly or completely dictionary encoded. It means that
dictionary indexes are saved in the data pages instead of the actual values. The
actual values are stored in the dictionary page. See details in Encodings.md.
The dictionary page must be placed at the first position of the column chunk. At
most one dictionary page can be placed in a column chunk.&lt;/p>
&lt;p>Additionally, files can contain an optional column index to allow readers to
skip pages more efficiently. See PageIndex.md for details and
the reasoning behind adding these to the format.&lt;/p></description></item><item><title>Docs: Error Recovery</title><link>/docs/file-format/data-pages/errorrecovery/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/data-pages/errorrecovery/</guid><description>
&lt;p>If the file metadata is corrupt, the file is lost. If the column metadata is corrupt,
that column chunk is lost (but column chunks for this column in other row groups are
okay). If a page header is corrupt, the remaining pages in that chunk are lost. If
the data within a page is corrupt, that page is lost. The file will be more
resilient to corruption with smaller row groups.&lt;/p>
&lt;p>Potential extension: With smaller row groups, the biggest issue is placing the file
metadata at the end. If an error happens while writing the file metadata, all the
data written will be unreadable. This can be fixed by writing the file metadata
every Nth row group.
Each file metadata would be cumulative and include all the row groups written so
far. Combining this with the strategy used for rc or avro files using sync markers,
a reader could recover partially written files.&lt;/p></description></item><item><title>Docs: Nulls</title><link>/docs/file-format/nulls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/nulls/</guid><description>
&lt;p>Nullity is encoded in the definition levels (which is run-length encoded). NULL values
are not encoded in the data. For example, in a non-nested schema, a column with 1000 NULLs
would be encoded with run-length encoding (0, 1000 times) for the definition levels and
nothing else.&lt;/p></description></item><item><title>Docs: Page Index</title><link>/docs/file-format/pageindex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/file-format/pageindex/</guid><description>
&lt;p>This document describes the format for column index pages in the Parquet
footer. These pages contain statistics for DataPages and can be used to skip
pages when scanning data in ordered and unordered columns.&lt;/p>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>In previous versions of the format, Statistics are stored for ColumnChunks in
ColumnMetaData and for individual pages inside DataPageHeader structs. When
reading pages, a reader had to process the page header to determine
whether the page could be skipped based on the statistics. This means the reader
had to access all pages in a column, thus likely reading most of the column
data from disk.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;ol>
&lt;li>Make both range scans and point lookups I/O efficient by allowing direct
access to pages based on their min and max values. In particular:
&lt;ul>
&lt;li>A single-row lookup in a row group based on the sort column of that row group
will only read one data page per the retrieved column.&lt;/li>
&lt;li>Range scans on the sort column will only need to read the exact data
pages that contain relevant data.&lt;/li>
&lt;li>Make other selective scans I/O efficient: if we have a very selective
predicate on a non-sorting column, for the other retrieved columns we
should only need to access data pages that contain matching rows.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>No additional decoding effort for scans without selective predicates, e.g.,
full-row group scans. If a reader determines that it does not need to read
the index data, it does not incur any overhead.&lt;/li>
&lt;li>Index pages for sorted columns use minimal storage by storing only the
boundary elements between pages.&lt;/li>
&lt;/ol>
&lt;h2 id="non-goals">Non-Goals&lt;/h2>
&lt;ul>
&lt;li>Support for the equivalent of secondary indices, i.e., an index structure
sorted on the key values over non-sorted data.&lt;/li>
&lt;/ul>
&lt;h2 id="technical-approach">Technical Approach&lt;/h2>
&lt;p>We add two new per-column structures to the row group metadata:&lt;/p>
&lt;ul>
&lt;li>ColumnIndex: this allows navigation to the pages of a column based on column
values and is used to locate data pages that contain matching values for a
scan predicate&lt;/li>
&lt;li>OffsetIndex: this allows navigation by row index and is used to retrieve
values for rows identified as matches via the ColumnIndex. Once rows of a
column are skipped, the corresponding rows in the other columns have to be
skipped. Hence the OffsetIndexes for each column in a RowGroup are stored
together.&lt;/li>
&lt;/ul>
&lt;p>The new index structures are stored separately from RowGroup, near the footer.&lt;br>
This is done so that a reader does not have to pay the I/O and deserialization
cost for reading them if it is not doing selective scans. The index structures'
location and length are stored in ColumnChunk.&lt;/p>
&lt;p>&lt;img alt="Page Index Layout" src="/images/PageIndexLayout.png">&lt;/p>
&lt;p>Some observations:&lt;/p>
&lt;ul>
&lt;li>We don&amp;rsquo;t need to record the lower bound for the first page and the upper
bound for the last page, because the row group Statistics can provide that.
We still include those for the sake of uniformity, and the overhead should be
negligible.&lt;/li>
&lt;li>We store lower and upper bounds for the values of each page. These may be the
actual minimum and maximum values found on a page, but can also be (more
compact) values that do not exist on a page. For example, instead of storing
&amp;ldquo;&amp;ldquo;Blart Versenwald III&amp;rdquo;, a writer may set &lt;code>min_values[i]=&amp;quot;B&amp;quot;&lt;/code>,
&lt;code>max_values[i]=&amp;quot;C&amp;quot;&lt;/code>. This allows writers to truncate large values and writers
should use this to enforce some reasonable bound on the size of the index
structures.&lt;/li>
&lt;li>Readers that support ColumnIndex should not also use page statistics. The
only reason to write page-level statistics when writing ColumnIndex structs
is to support older readers (not recommended).&lt;/li>
&lt;/ul>
&lt;p>For ordered columns, this allows a reader to find matching pages by performing
a binary search in &lt;code>min_values&lt;/code> and &lt;code>max_values&lt;/code>. For unordered columns, a
reader can find matching pages by sequentially reading &lt;code>min_values&lt;/code> and
&lt;code>max_values&lt;/code>.&lt;/p>
&lt;p>For range scans, this approach can be extended to return ranges of rows, page
indices, and page offsets to scan in each column. The reader can then
initialize a scanner for each column and fast forward them to the start row of
the scan.&lt;/p>
&lt;p>The &lt;code>min_values&lt;/code> and &lt;code>max_values&lt;/code> are calculated based on the &lt;code>column_orders&lt;/code>
field in the &lt;code>FileMetaData&lt;/code> struct of the footer.&lt;/p></description></item></channel></rss>